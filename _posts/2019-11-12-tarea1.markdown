---
layout: post
title: Tarea 1
date: 2019-08-13 09:00:00
description: Modelo de aprendizaje PAC
---
Para este primer cápitulo haremos una introducción al model de aprendizaje PAC.

Los primeros tres ejercicios fueron enviados a Canvas. Por lo que en esto se procederá únicamente al procedimiento númerico con python.

El objetivo de los siguientes puntos es obtener muestras

Se definirá una función para tener los conjuntos de entrenamiento, dicha función es la siguiente:


	def muestra(muestras):
   	return np.random.uniform(0,2,(muestras,2))

Para implementar una clase de hipótesis h: que vaya de X al conjunto de {0,1} y esto será para todo h en X.


	def claseH(l):
    	return np.linspace(0,1,round(1/l))
	
Ahora para calcular los errores de generalización se definirá la siguiente función en python.


	def ErrorG(hipotesis):
	    cota = 1-1/np.sqrt(2)
	    if hipotesis < cota:
		return 1/2-2*h+h**2
	    elif hipotesis > cota:
		return -1/2+2*h-h**2
	    else:
		return 0

Para obtener los rectangulos para los Erros de minimización Emperico se define la siguiente función:

	def ErrorMEmpirico(ClaseHipo,clase):
	   error = 1
	   EMRh = 0
	   for h in np.nditer(ClaseHipo):
		LError = ErrorEm(h,Clase) 
		  if LError < error:
		  error = LError
		  ErrorMEmpiricoh = h
	   return ErrorMEmpricoh

¿Qué dice en este caso el resultado de que las clases de hipótesis finitas son PAC aprendibles con complejidad de muestra log(|H|/δ)/ϵ?
Esto nos muestra el número de muestras que se necesita para que el error de generalización sea el mínimo, es decir, que dado un ϵ arbitrario, este siempre este bajo control teniedo la probabiblidad 1−δ. 




